{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpilnLgQCx0UWHxX0J+IXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swati-git/FineTuneLLM/blob/main/FineTuning_a_LLM_LIMA_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VptvdBhy_ymo"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.57.3  peft==0.5.0 trl==0.19.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch==2.9.0"
      ],
      "metadata": {
        "id": "6kYpe9X7FIp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "def check_gpu_and_load(model_name, required_memory_gb=16):\n",
        "    \"\"\"Check if GPU has enough memory before loading model\"\"\"\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"⚠️  No GPU available, will use CPU\")\n",
        "    else:\n",
        "        print(\"✓ GPU available\")\n",
        "\n",
        "    # Check each GPU\n",
        "    suitable_gpus = []\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        total_gb = props.total_memory / (1024**3)\n",
        "        reserved_gb = torch.cuda.memory_reserved(i) / (1024**3)\n",
        "        free_gb = total_gb - reserved_gb\n",
        "\n",
        "        print(f\"GPU {i} ({props.name}): {free_gb:.1f} GB free / {total_gb:.1f} GB total\")\n",
        "\n",
        "        if free_gb >= required_memory_gb:\n",
        "            suitable_gpus.append(i)\n",
        "\n",
        "    if not suitable_gpus:\n",
        "        print(f\"⚠️  No GPU with {required_memory_gb} GB free. Use device_map='auto'\")\n",
        "    else :\n",
        "      print(f\"✓ Loading on GPU {suitable_gpus[0]}\")\n",
        "    # return AutoModelForCausalLM.from_pretrained(\n",
        "    #     model_name,\n",
        "    #     device_map=f\"cuda:{suitable_gpus[0]}\",\n",
        "    #     torch_dtype=torch.float16\n",
        "    # )\n"
      ],
      "metadata": {
        "id": "9ME16Bbwyi6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Usage\n",
        "model = check_gpu_and_load(\"facebook/opt-1.3b\", required_memory_gb=16)"
      ],
      "metadata": {
        "id": "7_Ez21sh6IxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find the model specs\n",
        "This will help in configuring the memory and compute required"
      ],
      "metadata": {
        "id": "MdWDLD5OpaKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"facebook/opt-1.3b\")\n",
        "print(f\"Data type of the parameters: {config.dtype} \")\n",
        "print(f\"Model name: {config.model_type}\")\n",
        "print(f\"Hidden size: {config.hidden_size}\")\n",
        "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"Max sequence length: {config.max_position_embeddings}\")"
      ],
      "metadata": {
        "id": "ClV_nkx-HLEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We will load the model in bfloat16 datatype because, bfloat16 has a wider range than float16*"
      ],
      "metadata": {
        "id": "6WpWTrc7q9Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\",\n",
        "                                             torch_dtype=torch.bfloat16,\n",
        "                                             device_map = \"auto\")"
      ],
      "metadata": {
        "id": "vtag_0xJDXIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Calculate memory (in GB)\n",
        "    bytes_per_param = 2 if str(model.dtype) == \"torch.bfloat16\" else 4\n",
        "    memory_gb = (total_params * bytes_per_param) / (1024**3)\n",
        "\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Model size in memory: {memory_gb:.2f} GB\")\n",
        "    print(f\"Data type: {model.dtype}\")"
      ],
      "metadata": {
        "id": "yoP-mqwpQ9V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_model_size(model)"
      ],
      "metadata": {
        "id": "aYCj620npGic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "I_0UkLDTRLwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "4VbaTyp5RNvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rule of thumb: Need 3-4x model size for training (gradients, optimizer states, etc.)\n",
        "#2.6 GB model → need ~8-10 GB GPU for training"
      ],
      "metadata": {
        "id": "TE0EgllBN992"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
        "\n",
        "# ===== CHECK THESE =====\n",
        "print(f\"Vocab size (tokenizer): {len(tokenizer)}\")\n",
        "print(f\"Vocab size (model): {model.config.vocab_size}\")\n",
        "\n",
        "# These should match!\n",
        "#assert len(tokenizer) == model.config.vocab_size, \"Mismatch!\"\n",
        "\n",
        "# Check special tokens\n",
        "print(f\"Padding token: {tokenizer.pad_token}\")\n",
        "print(f\"EOS token: {tokenizer.eos_token}\")\n",
        "print(f\"BOS token: {tokenizer.bos_token}\")\n",
        "\n",
        "# Test tokenization\n",
        "sample = \"Write a product description for headphones\"\n",
        "tokens = tokenizer.encode(sample)\n",
        "print(f\"Sample tokenization: {tokens}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")"
      ],
      "metadata": {
        "id": "9rRxfy2iR-6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q deeplake==3.7.1"
      ],
      "metadata": {
        "id": "X6JYcqQbaAiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deeplake\n",
        "\n",
        "# Connect to the training and testing datasets\n",
        "ds = deeplake.load('hub://genai360/GAIR-lima-train-set')\n",
        "ds_test = deeplake.load('hub://genai360/GAIR-lima-test-set')"
      ],
      "metadata": {
        "id": "E8xBjYQGaeFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "QfvSTI72a3Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sample_text(example):\n",
        "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
        "    text = f\"Question: {example['question'].text()}\\n\\nAnswer: {example['answer'].text()}\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "0WgmJxCOdpMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q trl==0.26.2\n",
        "#https://github.com/unslothai/unsloth/issues/3057\n"
      ],
      "metadata": {
        "id": "xRgsFgEfgdw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Given that the model's max sequence length is 2048 tokens as per  \"{config.max_position_embeddings}\" we'll structure our dataset to match it.\n",
        "\n",
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "train_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    infinite=True,\n",
        "    seq_length=2048\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "6Y2VN58obOyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ss19x3zRNgJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "WbmS79Szn0Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "eval_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds_test,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    infinite=True,\n",
        "    seq_length=2048\n",
        ")"
      ],
      "metadata": {
        "id": "bZFWwQ6kNmMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Rank Selection Guidelines**\n",
        "\n",
        "Small models (< 1B parameters): 8-16\n",
        "\n",
        "Medium models (1B-10B): 16-32\n",
        "\n",
        "Large models (> 10B): 32-64"
      ],
      "metadata": {
        "id": "6YOKQWbsKOuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alpha-to-Rank Relationship**\n",
        "\n",
        "Typically set to r or 2 * r\n",
        "\n",
        "Higher alpha increases the adaptation's impact\n",
        "\n",
        "Lower alpha reduces the adaptation's influence"
      ],
      "metadata": {
        "id": "siJsGWZ5Kpkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "NbltBrBUEw9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wandb"
      ],
      "metadata": {
        "id": "5mVJfXm_teoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Initialize W&B\n",
        "wandb.init(\n",
        "    project=\"opt-finetuning\",\n",
        "    #name=\"OPT-fine_tuned-LIMA-CPU\",\n",
        "    config={\n",
        "        \"model\": \"facebook/opt-1.3b\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "haQ_Ci5mtjDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./OPT-fine_tuned-LIMA-CPU\",\n",
        "\n",
        "    # Training settings\n",
        "\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4, # Reduced batch size\n",
        "    per_device_eval_batch_size=4,  # Reduced batch size\n",
        "    learning_rate=1e-5,\n",
        "    dataloader_drop_last=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,\n",
        "\n",
        "    # Evaluation settings\n",
        "    #evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    # Logging settings\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=1,\n",
        "\n",
        "    #num_train_epochs=10,\n",
        "\n",
        "    # per_device_train_batch_size=1,\n",
        "    # per_device_eval_batch_size=1,\n",
        "    learning_rate=1e-4,\n",
        "\n",
        "    gradient_accumulation_steps=2, # Increased gradient accumulation steps\n",
        "    #bf16=True,\n",
        "    weight_decay=0.05,\n",
        "    run_name=\"OPT-fine_tuned-LIMA-CPU\",\n",
        "\n",
        "    # W&B integration (automatic!)\n",
        "    report_to=\"wandb\",\n",
        ")"
      ],
      "metadata": {
        "id": "xc1DsM-2JkTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    #packing=True,\n",
        ")"
      ],
      "metadata": {
        "id": "UjBAiSumNW0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU memory for each device\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    props = torch.cuda.get_device_properties(i)\n",
        "    total_memory = props.total_memory / (1024**3)  # Convert to GB\n",
        "    print(f\"GPU {i}: {props.name}, {total_memory:.2f} GB\")"
      ],
      "metadata": {
        "id": "HDV6CXH4ovE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "EZ-fE7oLN_Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pipdeptree"
      ],
      "metadata": {
        "id": "wlioLi9-cw7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pipdeptree -p transformers"
      ],
      "metadata": {
        "id": "YcB37my6c1Da"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}