{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOAf4luBeJNDBMD8hoYBtM7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swati-git/FineTuneLLM/blob/main/FineTuning_a_LLM_LIMA_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VptvdBhy_ymo"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.57.3  peft==0.5.0 trl==0.19.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch==2.9.0"
      ],
      "metadata": {
        "id": "6kYpe9X7FIp3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "def check_gpu_and_load(model_name, required_memory_gb=16):\n",
        "    \"\"\"Check if GPU has enough memory before loading model\"\"\"\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"⚠️  No GPU available, will use CPU\")\n",
        "    else:\n",
        "        print(\"✓ GPU available\")\n",
        "\n",
        "    # Check each GPU\n",
        "    suitable_gpus = []\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        total_gb = props.total_memory / (1024**3)\n",
        "        reserved_gb = torch.cuda.memory_reserved(i) / (1024**3)\n",
        "        free_gb = total_gb - reserved_gb\n",
        "\n",
        "        print(f\"GPU {i} ({props.name}): {free_gb:.1f} GB free / {total_gb:.1f} GB total\")\n",
        "\n",
        "        if free_gb >= required_memory_gb:\n",
        "            suitable_gpus.append(i)\n",
        "\n",
        "    if not suitable_gpus:\n",
        "        print(f\"⚠️  No GPU with {required_memory_gb} GB free. Use device_map='auto'\")\n",
        "    else :\n",
        "      print(f\"✓ Loading on GPU {suitable_gpus[0]}\")\n",
        "    # return AutoModelForCausalLM.from_pretrained(\n",
        "    #     model_name,\n",
        "    #     device_map=f\"cuda:{suitable_gpus[0]}\",\n",
        "    #     torch_dtype=torch.float16\n",
        "    # )\n"
      ],
      "metadata": {
        "id": "9ME16Bbwyi6Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "check_gpu_and_load(\"facebook/opt-1.3b\", required_memory_gb=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_Ez21sh6IxF",
        "outputId": "d6fcdc6f-967e-4b60-abc9-d8068e9a2434"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ GPU available\n",
            "GPU 0 (Tesla T4): 14.7 GB free / 14.7 GB total\n",
            "⚠️  No GPU with 16 GB free. Use device_map='auto'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find the model specs\n",
        "This will help in configuring the memory and compute required"
      ],
      "metadata": {
        "id": "MdWDLD5OpaKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"facebook/opt-1.3b\")\n",
        "print(f\"Data type of the parameters: {config.dtype} \")\n",
        "print(f\"Model name: {config.model_type}\")\n",
        "print(f\"Hidden size: {config.hidden_size}\")\n",
        "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"Max sequence length: {config.max_position_embeddings}\")"
      ],
      "metadata": {
        "id": "ClV_nkx-HLEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b4ed01-3bd1-44ee-8872-dcb21ab4031c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type of the parameters: torch.float16 \n",
            "Model name: opt\n",
            "Hidden size: 2048\n",
            "Number of layers: 24\n",
            "Vocabulary size: 50272\n",
            "Max sequence length: 2048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We will load the model in bfloat16 datatype because, bfloat16 has a wider range than float16*"
      ],
      "metadata": {
        "id": "6WpWTrc7q9Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\",\n",
        "                                             dtype=torch.bfloat16,\n",
        "                                             device_map = \"auto\")"
      ],
      "metadata": {
        "id": "vtag_0xJDXIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3791abb1-dc12-4b00-e91e-9a688a929376"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Calculate memory (in GB)\n",
        "    bytes_per_param = 2 if str(model.dtype) == \"torch.bfloat16\" else 4\n",
        "    memory_gb = (total_params * bytes_per_param) / (1024**3)\n",
        "\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Model size in memory: {memory_gb:.2f} GB\")\n",
        "    print(f\"Data type: {model.dtype}\")"
      ],
      "metadata": {
        "id": "yoP-mqwpQ9V8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_model_size(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYCj620npGic",
        "outputId": "6ef667ec-3c51-48de-fcc7-605a63a1d69b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 1,315,758,080\n",
            "Trainable parameters: 1,315,758,080\n",
            "Model size in memory: 2.45 GB\n",
            "Data type: torch.bfloat16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "I_0UkLDTRLwS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "4VbaTyp5RNvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73b5f57-92ec-47c4-f7af-30f1184a5852"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1315758080 || all params: 1315758080 || trainable%: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rule of thumb: Need 3-4x model size for training (gradients, optimizer states, etc.)\n",
        "#2.6 GB model → need ~8-10 GB GPU for training"
      ],
      "metadata": {
        "id": "TE0EgllBN992"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
        "\n",
        "# ===== CHECK THESE =====\n",
        "print(f\"Vocab size (tokenizer): {len(tokenizer)}\")\n",
        "print(f\"Vocab size (model): {model.config.vocab_size}\")\n",
        "\n",
        "# These should match!\n",
        "#assert len(tokenizer) == model.config.vocab_size, \"Mismatch!\"\n",
        "\n",
        "# Check special tokens\n",
        "print(f\"Padding token: {tokenizer.pad_token}\")\n",
        "print(f\"EOS token: {tokenizer.eos_token}\")\n",
        "print(f\"BOS token: {tokenizer.bos_token}\")\n",
        "\n",
        "# Test tokenization\n",
        "sample = \"Write a product description for headphones\"\n",
        "tokens = tokenizer.encode(sample)\n",
        "print(f\"Sample tokenization: {tokens}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")"
      ],
      "metadata": {
        "id": "9rRxfy2iR-6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198b6148-0472-4d04-a2c7-921549f1bef0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size (tokenizer): 50265\n",
            "Vocab size (model): 50272\n",
            "Padding token: <pad>\n",
            "EOS token: </s>\n",
            "BOS token: </s>\n",
            "Sample tokenization: [2, 45714, 10, 1152, 8194, 13, 15684]\n",
            "Number of tokens: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q deeplake==3.7.1"
      ],
      "metadata": {
        "id": "X6JYcqQbaAiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f65d88-3dfd-4dda-9c82-99ebf281a683"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/554.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/554.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m553.0/554.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.7/554.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
            "datasets 4.0.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import deeplake\n",
        "\n",
        "# Connect to the training and testing datasets\n",
        "ds = deeplake.load('hub://genai360/GAIR-lima-train-set')\n",
        "ds_test = deeplake.load('hub://genai360/GAIR-lima-test-set')"
      ],
      "metadata": {
        "id": "E8xBjYQGaeFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "332c3b86-627b-4aa4-afe2-8892c0c2f0c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'deeplake'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-765727059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdeeplake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Connect to the training and testing datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeeplake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hub://genai360/GAIR-lima-train-set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeeplake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hub://genai360/GAIR-lima-test-set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deeplake'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "QfvSTI72a3Yy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48361909-566d-47d2-beaa-94bb442ac147"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(path='hub://genai360/GAIR-lima-train-set', read_only=True, tensors=['answer', 'embeddings', 'question', 'source'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sample_text(example):\n",
        "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
        "    text = f\"Question: {example['question'].text()}\\n\\nAnswer: {example['answer'].text()}\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "0WgmJxCOdpMB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q trl==0.26.2\n",
        "#https://github.com/unslothai/unsloth/issues/3057\n"
      ],
      "metadata": {
        "id": "xRgsFgEfgdw0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Given that the model's max sequence length is 2048 tokens as per  \"{config.max_position_embeddings}\" we'll structure our dataset to match it.\n",
        "\n",
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "train_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    infinite=True,\n",
        "    seq_length=2048\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "6Y2VN58obOyN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ss19x3zRNgJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "WbmS79Szn0Iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e49854-2b08-4aa4-9e05-6d02eff1831e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<trl.trainer.utils.ConstantLengthDataset at 0x78d5d74e4710>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "eval_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds_test,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    infinite=True,\n",
        "    seq_length=2048\n",
        ")"
      ],
      "metadata": {
        "id": "bZFWwQ6kNmMH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Rank Selection Guidelines**\n",
        "\n",
        "Small models (< 1B parameters): 8-16\n",
        "\n",
        "Medium models (1B-10B): 16-32\n",
        "\n",
        "Large models (> 10B): 32-64"
      ],
      "metadata": {
        "id": "6YOKQWbsKOuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alpha-to-Rank Relationship**\n",
        "\n",
        "Typically set to r or 2 * r\n",
        "\n",
        "Higher alpha increases the adaptation's impact\n",
        "\n",
        "Lower alpha reduces the adaptation's influence"
      ],
      "metadata": {
        "id": "siJsGWZ5Kpkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "NbltBrBUEw9q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wandb"
      ],
      "metadata": {
        "id": "5mVJfXm_teoQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Initialize W&B\n",
        "wandb.init(\n",
        "    project=\"opt-finetuning\",\n",
        "    #name=\"OPT-fine_tuned-LIMA-CPU\",\n",
        "    config={\n",
        "        \"model\": \"facebook/opt-1.3b\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "haQ_Ci5mtjDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./OPT-fine_tuned-LIMA-CPU\",\n",
        "\n",
        "    # Training settings\n",
        "\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=1e-5,\n",
        "    dataloader_drop_last=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,\n",
        "\n",
        "    # Evaluation settings\n",
        "    #evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    # Logging settings\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=1,\n",
        "\n",
        "    #num_train_epochs=10,\n",
        "\n",
        "    # per_device_train_batch_size=1,\n",
        "    # per_device_eval_batch_size=1,\n",
        "    #learning_rate=1e-4,\n",
        "\n",
        "    gradient_accumulation_steps=4,\n",
        "    bf16=True,\n",
        "    weight_decay=0.05,\n",
        "    run_name=\"OPT-fine_tuned-LIMA-CPU\",\n",
        "\n",
        "    # W&B integration (automatic!)\n",
        "    report_to=\"wandb\",\n",
        ")"
      ],
      "metadata": {
        "id": "xc1DsM-2JkTX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.lm_head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPJ83vCpwhnj",
        "outputId": "4fdc3e04-eea1-4ee1-b041-080a5b0911f5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=2048, out_features=50272, bias=False)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4A4mw4_cuY1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Note: The initial run caused \"CUDA out of memory\" error as the training started because all the paramters of this model are trainable."
      ],
      "metadata": {
        "id": "J49RfYsaubUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n"
      ],
      "metadata": {
        "id": "y1fBPn98uWNA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "XP8XKZLhwdC2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "#print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sjuEEpQwHgX",
        "outputId": "2473e12c-976d-4402-e5c4-7d380b08f4c3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,145,728 || all params: 1,318,903,808 || trainable%: 0.23851079820371554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    #packing=True,\n",
        ")"
      ],
      "metadata": {
        "id": "UjBAiSumNW0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef55177-ec80-4dc2-9806-1df64b2cbc67"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check GPU memory for each device\n",
        "# for i in range(torch.cuda.device_count()):\n",
        "#     props = torch.cuda.get_device_properties(i)\n",
        "#     total_memory = props.total_memory / (1024**3)  # Convert to GB\n",
        "#     print(f\"GPU {i}: {props.name}, {total_memory:.2f} GB\")"
      ],
      "metadata": {
        "id": "HDV6CXH4ovE2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Check memory before starting\n",
        "print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
        "print(f\"GPU memory reserved: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXQVGpTJ0udp",
        "outputId": "b4cc2a10-c918-44e6-f935-bc022700b52e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory allocated: 2.46 GB\n",
            "GPU memory reserved: 2.53 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "EZ-fE7oLN_Ux",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd15cbe3-f931-4edd-8b64-48a55da64d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids = [torch.tensor(example[\"input_ids\"]) for example in examples]\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:204: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(example[\"labels\"]) for example in examples]\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 29/195 1:05:46 < 6:44:22, 0.01 it/s, Epoch 0.43/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.424600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.504900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.419300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.452900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.317000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.486200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.500900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.401000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.430400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.382900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.440700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.437600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.433400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.422000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.424700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.465000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.409700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.445800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.465800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.471600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.391100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.448800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.347900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.408700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.374400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pipdeptree"
      ],
      "metadata": {
        "id": "wlioLi9-cw7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pipdeptree -p transformers"
      ],
      "metadata": {
        "id": "YcB37my6c1Da"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}