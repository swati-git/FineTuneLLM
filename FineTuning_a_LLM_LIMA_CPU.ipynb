{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpmKFuPUAitmBqbPSxKDLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swati-git/FineTuneLLM/blob/main/FineTuning_a_LLM_LIMA_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VptvdBhy_ymo"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.57.3  peft==0.5.0 trl==0.19.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch==2.9.0"
      ],
      "metadata": {
        "id": "6kYpe9X7FIp3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"facebook/opt-1.3b\")\n",
        "print(config.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClV_nkx-HLEa",
        "outputId": "505d988c-8eab-429a-ba41-384a41b144e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model name: {config.model_type}\")\n",
        "print(f\"Hidden size: {config.hidden_size}\")           # 2048\n",
        "print(f\"Number of layers: {config.num_hidden_layers}\") # 24\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")        # 50272\n",
        "print(f\"Max sequence length: {config.max_position_embeddings}\") # 2048"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxBXNk-FQZCm",
        "outputId": "8b58a5ba-f745-4e85-f470-88a7ecf8ffc8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name: opt\n",
            "Hidden size: 2048\n",
            "Number of layers: 24\n",
            "Vocabulary size: 50272\n",
            "Max sequence length: 2048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtag_0xJDXIM",
        "outputId": "9946f9f2-d663-4169-dd9a-746dbbee7fa1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Calculate memory (in GB)\n",
        "    bytes_per_param = 2 if str(model.dtype) == \"torch.bfloat16\" else 4\n",
        "    memory_gb = (total_params * bytes_per_param) / (1024**3)\n",
        "\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Model size in memory: {memory_gb:.2f} GB\")\n",
        "    print(f\"Data type: {model.dtype}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yoP-mqwpQ9V8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "I_0UkLDTRLwS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VbaTyp5RNvR",
        "outputId": "1f8f35c8-7167-427f-f772-506c2b87af16"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1315758080 || all params: 1315758080 || trainable%: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_model_size(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYK3Z529Q-q_",
        "outputId": "681706ea-f8f2-4efd-da54-2c3d0841fcd1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 1,315,758,080\n",
            "Trainable parameters: 1,315,758,080\n",
            "Model size in memory: 2.45 GB\n",
            "Data type: torch.bfloat16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rule of thumb: Need 3-4x model size for training (gradients, optimizer states, etc.)\n",
        "#2.6 GB model â†’ need ~8-10 GB GPU for training"
      ],
      "metadata": {
        "id": "TE0EgllBN992"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
        "\n",
        "# ===== CHECK THESE =====\n",
        "print(f\"Vocab size (tokenizer): {len(tokenizer)}\")\n",
        "print(f\"Vocab size (model): {model.config.vocab_size}\")\n",
        "\n",
        "# These should match!\n",
        "#assert len(tokenizer) == model.config.vocab_size, \"Mismatch!\"\n",
        "\n",
        "# Check special tokens\n",
        "print(f\"Padding token: {tokenizer.pad_token}\")\n",
        "print(f\"EOS token: {tokenizer.eos_token}\")\n",
        "print(f\"BOS token: {tokenizer.bos_token}\")\n",
        "\n",
        "# Test tokenization\n",
        "sample = \"Write a product description for headphones\"\n",
        "tokens = tokenizer.encode(sample)\n",
        "print(f\"Sample tokenization: {tokens}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rRxfy2iR-6G",
        "outputId": "9e920559-b6c9-4081-a14c-bdfb2ca0bd61"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size (tokenizer): 50265\n",
            "Vocab size (model): 50272\n",
            "Padding token: <pad>\n",
            "EOS token: </s>\n",
            "BOS token: </s>\n",
            "Sample tokenization: [2, 45714, 10, 1152, 8194, 13, 15684]\n",
            "Number of tokens: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q deeplake==3.7.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6JYcqQbaAiy",
        "outputId": "2b70f54e-f417-4262-c34e-8032345bb649"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
            "datasets 4.0.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import deeplake\n",
        "\n",
        "# Connect to the training and testing datasets\n",
        "ds = deeplake.load('hub://genai360/GAIR-lima-train-set')\n",
        "ds_test = deeplake.load('hub://genai360/GAIR-lima-test-set')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8xBjYQGaeFv",
        "outputId": "2a0660a5-d0e3-4cc1-a70e-9c60e3aaf4c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.4.4) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n",
            "\\"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/deeplake/core/fast_forwarding.py:43: UserWarning: Loading a dataset that was created or updated with a newer version of deeplake. This could lead to corruption or unexpected errors! Dataset version: 3.7.2, current deeplake version: 3.7.1. It's recommended that you update to a version of deeplake >= 3.7.2.\n",
            "  warnings.warn(\n",
            "\n",
            "|"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/genai360/GAIR-lima-train-set\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://genai360/GAIR-lima-train-set loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/genai360/GAIR-lima-test-set\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "-"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://genai360/GAIR-lima-test-set loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r\r\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfvSTI72a3Yy",
        "outputId": "60ed1f83-f7fc-48e6-ce3e-0268515729d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(path='hub://genai360/GAIR-lima-train-set', read_only=True, tensors=['answer', 'embeddings', 'question', 'source'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pipdeptree"
      ],
      "metadata": {
        "id": "wlioLi9-cw7u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pipdeptree -p transformers"
      ],
      "metadata": {
        "id": "YcB37my6c1Da"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sample_text(example):\n",
        "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
        "    text = f\"Question: {example['question'].text()}\\n\\nAnswer: {example['answer'].text()}\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "0WgmJxCOdpMB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q trl==0.26.2\n",
        "#https://github.com/unslothai/unsloth/issues/3057\n"
      ],
      "metadata": {
        "id": "xRgsFgEfgdw0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Given that the model's max sequence length is 2048 tokens as per  \"{config.max_position_embeddings}\" we'll structure our dataset to match it.\n",
        "\n",
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "train_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    infinite=True,\n",
        "    seq_length=2048\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "6Y2VN58obOyN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ss19x3zRNgJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbmS79Szn0Iq",
        "outputId": "ff27575f-2c2d-4a3f-96db-5042fa357378"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<trl.trainer.utils.ConstantLengthDataset at 0x79eeb41cd6a0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "eval_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds_test,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    infinite=True,\n",
        "    seq_length=2048\n",
        ")"
      ],
      "metadata": {
        "id": "bZFWwQ6kNmMH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Rank Selection Guidelines**\n",
        "\n",
        "Small models (< 1B parameters): 8-16\n",
        "\n",
        "Medium models (1B-10B): 16-32\n",
        "\n",
        "Large models (> 10B): 32-64"
      ],
      "metadata": {
        "id": "6YOKQWbsKOuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alpha-to-Rank Relationship**\n",
        "\n",
        "Typically set to r or 2 * r\n",
        "\n",
        "Higher alpha increases the adaptation's impact\n",
        "\n",
        "Lower alpha reduces the adaptation's influence"
      ],
      "metadata": {
        "id": "siJsGWZ5Kpkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "NbltBrBUEw9q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./OPT-fine_tuned-LIMA-CPU\",\n",
        "    dataloader_drop_last=True,\n",
        "    #evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,\n",
        "    gradient_accumulation_steps=1,\n",
        "    #bf16=True,\n",
        "    weight_decay=0.05,\n",
        "    run_name=\"OPT-fine_tuned-LIMA-CPU\",\n",
        "    report_to=\"wandb\",\n",
        ")"
      ],
      "metadata": {
        "id": "xc1DsM-2JkTX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    #packing=True,\n",
        ")"
      ],
      "metadata": {
        "id": "UjBAiSumNW0B"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "EZ-fE7oLN_Ux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}